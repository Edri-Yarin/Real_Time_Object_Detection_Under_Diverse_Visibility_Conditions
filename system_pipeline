{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23005,"status":"ok","timestamp":1741619583418,"user":{"displayName":"Yarin Edri","userId":"16106484749537069937"},"user_tz":-120},"id":"fEUVigjq4qD5","outputId":"067a6299-be3c-49fe-afd5-57bd879457b9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":112347,"status":"ok","timestamp":1741430175718,"user":{"displayName":"Yarin Edri","userId":"16106484749537069937"},"user_tz":-120},"id":"AwkjK3-88yw4","outputId":"7e3cb759-f1ff-4c7a-d983-c0348039831b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ultralytics\n","  Downloading ultralytics-8.3.85-py3-none-any.whl.metadata (35 kB)\n","Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.26.4)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.1.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.13.1)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.5.1+cu124)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.20.1+cu124)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n","Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.56.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n","Downloading ultralytics-8.3.85-py3-none-any.whl (922 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m922.2/922.2 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n","Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.85 ultralytics-thop-2.0.14\n"]}],"source":["!pip install ultralytics\n"]},{"cell_type":"markdown","source":[],"metadata":{"id":"ugWJ7eBNjzfG"}},{"cell_type":"code","source":["import os\n","import random\n","import time\n","import numpy as np\n","import pandas as pd\n","import cv2\n","import torch\n","import torch.nn as nn\n","from PIL import Image\n","from ultralytics import YOLO\n","from torchvision import transforms\n","import matplotlib.pyplot as plt\n","from google.colab.patches import cv2_imshow\n","import yaml\n","import glob\n","from sklearn.model_selection import train_test_split\n","\n","# -------------------------------\n","# 1. Helper Functions\n","# -------------------------------\n","\n","def load_gt_boxes(label_path, image_size):\n","    \"\"\"\n","    Loads ground-truth boxes from a YOLO-format label file.\n","    Each line: class x_center y_center width height (normalized).\n","    Returns a numpy array of boxes: [x1, y1, x2, y2].\n","    \"\"\"\n","    boxes = []\n","    if os.path.exists(label_path):\n","        with open(label_path, \"r\") as f:\n","            for line in f:\n","                parts = line.strip().split()\n","                if len(parts) == 5:\n","                    _, xc, yc, w, h = map(float, parts)\n","                    W, H = image_size\n","                    x1 = (xc - w/2) * W\n","                    y1 = (yc - h/2) * H\n","                    x2 = (xc + w/2) * W\n","                    y2 = (yc + h/2) * H\n","                    boxes.append([x1, y1, x2, y2])\n","    return np.array(boxes)\n","\n","def create_dataset_yaml(condition, images_folder, nc=10, names=[\"class0\", \"class1\", \"class2\", \"class3\", \"class4\",\n","                                                              \"class5\", \"class6\", \"class7\", \"class8\", \"class9\"]):\n","    \"\"\"\n","    Creates a temporary YAML file for evaluation.\n","    Sets both \"train\" and \"val\" keys to the validation folder and includes \"nc\" and \"names\".\n","    \"\"\"\n","    parent_dir = os.path.dirname(os.path.dirname(images_folder))\n","    data = {\n","         \"path\": parent_dir,\n","         \"train\": os.path.join(\"images\", \"val\"),\n","         \"val\": os.path.join(\"images\", \"val\"),\n","         \"nc\": nc,\n","         \"names\": names\n","    }\n","    yaml_path = f\"temp_{condition}.yaml\"\n","    with open(yaml_path, \"w\") as f:\n","         yaml.dump(data, f)\n","    return yaml_path\n","\n","def evaluate_model_on_dataset(model, yaml_path):\n","    \"\"\"\n","    Uses YOLOv8's built-in validation method to evaluate the model on the dataset defined in yaml_path.\n","    Returns a dictionary with mAP (using map50), precision, recall, and F1 score.\n","    Note: We access metric values without calling them as functions.\n","    \"\"\"\n","    metrics = model.val(data=yaml_path, verbose=False)\n","    try:\n","        mAP = metrics.box.map50   # Access mAP at IoU=0.5\n","        prec = metrics.box.mp     # Mean precision\n","        rec = metrics.box.mr      # Mean recall\n","    except Exception as e:\n","        print(\"Error evaluating metrics:\", e)\n","        mAP, prec, rec = 0, 0, 0\n","    f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0\n","    return {\"mAP\": mAP, \"precision\": prec, \"recall\": rec, \"f1\": f1}\n","\n","def measure_inference_time(model, image_paths):\n","    \"\"\"\n","    Measures average inference time per image for the given model.\n","    \"\"\"\n","    times = []\n","    for img_path in image_paths:\n","        start = time.time()\n","        _ = model(img_path, verbose=False)\n","        times.append(time.time() - start)\n","    return np.mean(times)\n","\n","def measure_classification_time(classifier, image_paths):\n","    \"\"\"\n","    Measures average classification time per image for the visibility classifier.\n","    \"\"\"\n","    times = []\n","    for img_path in image_paths:\n","        start = time.time()\n","        _ = classify_visibility(img_path, classifier)\n","        times.append(time.time() - start)\n","    return np.mean(times)\n","\n","# -------------------------------\n","# 2. Visibility Classifier & Model Loading\n","# -------------------------------\n","\n","class VisibilityClassifier(nn.Module):\n","    def __init__(self, num_classes=5):  # classes: fog, rain, sand, snow, good_condition_best\n","        super(VisibilityClassifier, self).__init__()\n","        self.model = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(128*128*3, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, num_classes)\n","        )\n","    def forward(self, x):\n","        return self.model(x)\n","\n","def load_visibility_model(model_path):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model = VisibilityClassifier().to(device)\n","    state_dict = torch.load(model_path, map_location=device)\n","    new_state_dict = { k if k.startswith(\"model.\") else f\"model.{k}\": v for k, v in state_dict.items() }\n","    model.load_state_dict(new_state_dict)\n","    model.eval()\n","    return model\n","\n","def classify_visibility(image_path, model):\n","    transform = transforms.Compose([\n","        transforms.Resize((128, 128)),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485,0.456,0.406],\n","                             [0.229,0.224,0.225])\n","    ])\n","    image = Image.open(image_path).convert(\"RGB\")\n","    image = transform(image).unsqueeze(0)\n","    image = image.to(next(model.parameters()).device)\n","    with torch.no_grad():\n","        output = model(image)\n","        pred = torch.argmax(output, dim=1).item()\n","    condition_map = [\"fog\", \"rain\", \"sand\", \"snow\", \"good_condition_best\"]\n","    return condition_map[pred]\n","\n","# -------------------------------\n","# 3. YOLO Model Loading\n","# -------------------------------\n","\n","BASE_WEIGHTS_PATH = \"/content/drive/MyDrive/DEEP_LEARNING/FinalProject/datasets/DAWN/weights\"\n","FILE_MAPPING = {\n","    \"fog\": (\"yolov8_fog\", \"yolov8_fog_best.pt\"),\n","    \"rain\": (\"yolov8_rain\", \"yolov8_rain_best.pt\"),\n","    \"sand\": (\"yolov8_sand\", \"yolov8_sand_best.pt\"),\n","    \"snow\": (\"yolov8_snow\", \"yolov8_snow_best.pt\"),\n","    \"good_condition_best\": (\"yolov8_good_condition\", \"yolov8_good_condition_best.pt\")\n","}\n","\n","def load_submodel(condition):\n","    if condition not in FILE_MAPPING:\n","        raise ValueError(f\"No submodel mapping for condition={condition}\")\n","    folder, filename = FILE_MAPPING[condition]\n","    model_path = os.path.join(BASE_WEIGHTS_PATH, folder, filename)\n","    if not os.path.exists(model_path):\n","        raise FileNotFoundError(f\"Submodel not found at {model_path}\")\n","    return YOLO(model_path)\n","\n","# Load the general model (trained on all conditions)\n","GENERAL_MODEL_PATH = os.path.join(BASE_WEIGHTS_PATH, \"yolov8_genral_case\", \"yolov8_genral_case_best.pt\")\n","general_model = YOLO(GENERAL_MODEL_PATH)\n","\n","# -------------------------------\n","# 4. YOLO Inference Function\n","# -------------------------------\n","\n","def run_inference(image_path, model):\n","    results = model(image_path, verbose=False)\n","    if results[0].boxes is not None:\n","        boxes = results[0].boxes.xyxy.cpu().numpy()\n","        conf = results[0].boxes.conf.cpu().numpy()\n","        cls_ids = results[0].boxes.cls.cpu().numpy().astype(int)\n","    else:\n","        boxes, conf, cls_ids = [], [], []\n","    return {\"boxes\": boxes, \"scores\": conf, \"class_ids\": cls_ids}\n","\n","# -------------------------------\n","# 5. Visualization Functions\n","# -------------------------------\n","\n","def visualize_comparison_grid(image_paths, sub_detections, gen_detections, sub_model_names, gen_model_names, condition, title_suffix=\"\"):\n","    \"\"\"\n","    Creates a grid with 2 rows and N columns:\n","      - Top row: images with submodel detections.\n","      - Bottom row: same images with general model detections.\n","    \"\"\"\n","    N = len(image_paths)\n","    plt.figure(figsize=(4 * N, 8))\n","    for i, img_path in enumerate(image_paths):\n","        orig = cv2.imread(img_path)\n","        orig_rgb = cv2.cvtColor(orig, cv2.COLOR_BGR2RGB)\n","        img_sub = orig_rgb.copy()\n","        img_gen = orig_rgb.copy()\n","\n","        # Draw submodel detections (green)\n","        for bbox, score, cls in zip(sub_detections[i][\"boxes\"], sub_detections[i][\"scores\"], sub_detections[i][\"class_ids\"]):\n","            x1, y1, x2, y2 = map(int, bbox)\n","            label = f\"{sub_model_names[cls]}: {score:.2f}\"\n","            cv2.rectangle(img_sub, (x1, y1), (x2, y2), (0, 255, 0), 2)\n","            cv2.putText(img_sub, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n","\n","        # Draw general model detections (red)\n","        for bbox, score, cls in zip(gen_detections[i][\"boxes\"], gen_detections[i][\"scores\"], gen_detections[i][\"class_ids\"]):\n","            x1, y1, x2, y2 = map(int, bbox)\n","            label = f\"{gen_model_names[cls]}: {score:.2f}\"\n","            cv2.rectangle(img_gen, (x1, y1), (x2, y2), (255, 0, 0), 2)\n","            cv2.putText(img_gen, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n","\n","        plt.subplot(2, N, i+1)\n","        plt.imshow(img_sub)\n","        plt.title(f\"Submodel ({condition}) {title_suffix}\")\n","        plt.axis(\"off\")\n","\n","        plt.subplot(2, N, N + i+1)\n","        plt.imshow(img_gen)\n","        plt.title(f\"General Model {title_suffix}\")\n","        plt.axis(\"off\")\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","def select_and_visualize_top_difference(condition, images_folder, num_examples=4, conf_threshold=0.25):\n","    \"\"\"\n","    For a given condition, computes the difference in the count of detections (with confidence >= conf_threshold)\n","    between the submodel and the general model for each image.\n","    Selects and displays the top 'num_examples' images where the submodel detects significantly more objects.\n","    \"\"\"\n","    sub_model = load_submodel(condition)\n","    sub_names = sub_model.names\n","    gen_names = general_model.names\n","\n","    image_list = [os.path.join(images_folder, f) for f in os.listdir(images_folder) if f.lower().endswith(\".jpg\")]\n","\n","    differences = []\n","    for img in image_list:\n","        det_sub = run_inference(img, sub_model)\n","        det_gen = run_inference(img, general_model)\n","        count_sub = np.sum(det_sub[\"scores\"] >= conf_threshold)\n","        count_gen = np.sum(det_gen[\"scores\"] >= conf_threshold)\n","        diff = count_sub - count_gen\n","        differences.append((img, diff, det_sub, det_gen))\n","\n","    # Filter to only images where sub-model detects more objects\n","    differences = [d for d in differences if d[1] > 0]\n","    differences.sort(key=lambda x: x[1], reverse=True)\n","\n","    best = differences[:num_examples]\n","    if not best:\n","        print(f\"No images found where {condition} submodel detected more objects than the general model.\")\n","        return\n","    best_imgs = [x[0] for x in best]\n","    sub_dets = [x[2] for x in best]\n","    gen_dets = [x[3] for x in best]\n","\n","    visualize_comparison_grid(best_imgs, sub_dets, gen_dets, sub_names, gen_names, condition, title_suffix=\"(Top Difference)\")\n","\n","def select_and_visualize_best(condition, images_folder, labels_folder, num_examples=4):\n","    \"\"\"\n","    For a given condition, selects a sample of images (here, the first 'num_examples' from a random sample)\n","    for side-by-side visualization of submodel vs. general model predictions.\n","    \"\"\"\n","    sub_model = load_submodel(condition)\n","    sub_names = sub_model.names\n","    gen_names = general_model.names\n","\n","    image_list = [os.path.join(images_folder, f) for f in os.listdir(images_folder) if f.lower().endswith(\".jpg\")]\n","    sampled_images = random.sample(image_list, min(50, len(image_list)))\n","    best_comparisons = sampled_images[:num_examples]\n","\n","    sub_dets = [run_inference(img, sub_model) for img in best_comparisons]\n","    gen_dets = [run_inference(img, general_model) for img in best_comparisons]\n","\n","    visualize_comparison_grid(best_comparisons, sub_dets, gen_dets, sub_names, gen_names, condition, title_suffix=\"(Best Examples)\")\n","\n","# -------------------------------\n","# 6. Main Evaluation Pipeline: Create Summary Table & Visualize\n","# -------------------------------\n","\n","# Define validation folders for each condition (images and labels)\n","CONDITIONS = {\n","    \"fog\": {\n","        \"images\": \"/content/drive/MyDrive/DEEP_LEARNING/FinalProject/datasets/DAWN/Fog/images/val\",\n","        \"labels\": \"/content/drive/MyDrive/DEEP_LEARNING/FinalProject/datasets/DAWN/Fog/labels/val\"\n","    },\n","    \"rain\": {\n","        \"images\": \"/content/drive/MyDrive/DEEP_LEARNING/FinalProject/datasets/DAWN/Rain/images/val\",\n","        \"labels\": \"/content/drive/MyDrive/DEEP_LEARNING/FinalProject/datasets/DAWN/Rain/labels/val\"\n","    },\n","    \"sand\": {\n","        \"images\": \"/content/drive/MyDrive/DEEP_LEARNING/FinalProject/datasets/DAWN/Sand/images/val\",\n","        \"labels\": \"/content/drive/MyDrive/DEEP_LEARNING/FinalProject/datasets/DAWN/Sand/labels/val\"\n","    },\n","    \"snow\": {\n","        \"images\": \"/content/drive/MyDrive/DEEP_LEARNING/FinalProject/datasets/DAWN/Snow/images/val\",\n","        \"labels\": \"/content/drive/MyDrive/DEEP_LEARNING/FinalProject/datasets/DAWN/Snow/labels/val\"\n","    },\n","    \"good_condition_best\": {\n","        \"images\": \"/content/drive/MyDrive/DEEP_LEARNING/FinalProject/datasets/DAWN/good_condition/images/val\",\n","        \"labels\": \"/content/drive/MyDrive/DEEP_LEARNING/FinalProject/datasets/DAWN/good_condition/labels/val\"\n","    }\n","}\n","\n","NUM_SAMPLE_IMAGES = 350  # number of images to sample for evaluation\n","results_records = []\n","\n","# Load the visibility classifier\n","visibility_model_path = \"/content/drive/MyDrive/DEEP_LEARNING/FinalProject/datasets/DAWN/weights/class_model.pth\"\n","visibility_model = load_visibility_model(visibility_model_path)\n","\n","# For each condition, evaluate the models and store results\n","for cond, paths in CONDITIONS.items():\n","    print(f\"\\n=== Evaluating condition: {cond.upper()} ===\")\n","    images_folder = paths[\"images\"]\n","    labels_folder = paths[\"labels\"]\n","\n","    # Create a temporary YAML file for dataset evaluation with required keys\n","    yaml_path = create_dataset_yaml(cond, images_folder, nc=10, names=[\"class0\", \"class1\", \"class2\", \"class3\", \"class4\",\n","                                                                      \"class5\", \"class6\", \"class7\", \"class8\", \"class9\"])\n","\n","    # Evaluate the condition-specific submodel and general model using YOLO's validation method\n","    correct_submodel = load_submodel(cond)\n","    correct_metrics = evaluate_model_on_dataset(correct_submodel, yaml_path)\n","    general_metrics = evaluate_model_on_dataset(general_model, yaml_path)\n","\n","    # For the full pipeline, we assume it ideally uses the correct submodel\n","    full_pipeline_metrics = evaluate_model_on_dataset(load_submodel(cond), yaml_path)\n","\n","    # Measure inference times on a sample of images\n","    image_list = [os.path.join(images_folder, f) for f in os.listdir(images_folder) if f.lower().endswith(\".jpg\")]\n","    sample_images = random.sample(image_list, min(NUM_SAMPLE_IMAGES, len(image_list)))\n","    correct_inference_time = measure_inference_time(correct_submodel, sample_images)\n","    general_inference_time = measure_inference_time(general_model, sample_images)\n","    classification_time = measure_classification_time(visibility_model, sample_images)\n","    full_pipeline_inference_time = classification_time + correct_inference_time\n","\n","    # Compute classification accuracy for the visibility classifier over the sample\n","    correct_classifications = 0\n","    for img_path in sample_images:\n","        predicted_cond = classify_visibility(img_path, visibility_model)\n","        if predicted_cond == cond:\n","            correct_classifications += 1\n","    classification_accuracy = correct_classifications / len(sample_images) if sample_images else 0.0\n","\n","    results_records.append({\n","         \"Condition\": cond,\n","         \"Full Pipeline (mAP)\": full_pipeline_metrics[\"mAP\"],\n","         \"Full Pipeline (Classification Accuracy)\": classification_accuracy,\n","         \"Full Pipeline (Inference Time, sec)\": full_pipeline_inference_time,\n","         \"Correct Sub-model (mAP)\": correct_metrics[\"mAP\"],\n","         \"Correct Sub-model (Precision)\": correct_metrics[\"precision\"],\n","         \"Correct Sub-model (Recall)\": correct_metrics[\"recall\"],\n","         \"Correct Sub-model (F1)\": correct_metrics[\"f1\"],\n","         \"Correct Sub-model (Inference Time, sec)\": correct_inference_time,\n","         \"General Model (mAP)\": general_metrics[\"mAP\"],\n","         \"General Model (Precision)\": general_metrics[\"precision\"],\n","         \"General Model (Recall)\": general_metrics[\"recall\"],\n","         \"General Model (F1)\": general_metrics[\"f1\"],\n","         \"General Model (Inference Time, sec)\": general_inference_time\n","    })\n","\n","df_results = pd.DataFrame(results_records)\n","print(\"\\n=== Summary Table ===\")\n","print(df_results)\n","df_results.to_csv(\"comparison_results.csv\", index=False)\n","\n","# -------------------------------\n","# Visualization:\n","# 1. Visualize images where the submodel detects more objects than the general model.\n","# -------------------------------\n","for cond, paths in CONDITIONS.items():\n","    print(f\"\\n--- Visualizing images where submodel detects more objects for condition: {cond.upper()} ---\")\n","    select_and_visualize_top_difference(cond, paths[\"images\"], num_examples=6, conf_threshold=0.25)\n","\n","# -------------------------------\n","# 2. Visualize best examples for qualitative comparison.\n","# -------------------------------\n","for cond, paths in CONDITIONS.items():\n","    print(f\"\\n--- Visualizing best examples for condition: {cond.upper()} ---\")\n","    select_and_visualize_best(cond, paths[\"images\"], paths[\"labels\"], num_examples=6)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1y5Hu4yifYGyeNqU752RypKxV5OKnNjG9"},"id":"-qoU1kSllC5d","executionInfo":{"status":"ok","timestamp":1741438714698,"user_tz":-120,"elapsed":492953,"user":{"displayName":"Yarin Edri","userId":"16106484749537069937"}},"outputId":"3ba97465-7198-42b2-d7bb-4b8d15f5e91c"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["find good photos\n"],"metadata":{"id":"KIdFbnVbrhOH"}},{"cell_type":"code","source":["\n","import os\n","import random\n","import time\n","import numpy as np\n","import cv2\n","import torch\n","import torch.nn as nn\n","from PIL import Image\n","from ultralytics import YOLO\n","from torchvision import transforms\n","import matplotlib.pyplot as plt\n","import yaml\n","import glob\n","from sklearn.model_selection import train_test_split\n","\n","# -------------------------------\n","# 1. Helper Functions for Metrics and AP Calculation\n","# -------------------------------\n","\n","def load_gt_boxes(label_path, image_size):\n","    \"\"\"\n","    Loads ground-truth boxes from a YOLO-format label file.\n","    Each line: class x_center y_center width height (normalized).\n","    Returns a numpy array of boxes: [x1, y1, x2, y2].\n","    \"\"\"\n","    boxes = []\n","    if os.path.exists(label_path):\n","        with open(label_path, \"r\") as f:\n","            for line in f:\n","                parts = line.strip().split()\n","                if len(parts) == 5:\n","                    _, xc, yc, w, h = map(float, parts)\n","                    W, H = image_size\n","                    x1 = (xc - w/2) * W\n","                    y1 = (yc - h/2) * H\n","                    x2 = (xc + w/2) * W\n","                    y2 = (yc + h/2) * H\n","                    boxes.append([x1, y1, x2, y2])\n","    return np.array(boxes)\n","\n","def iou(box1, box2):\n","    \"\"\"\n","    Computes Intersection over Union (IoU) for two boxes.\n","    Boxes are in the format: [x1, y1, x2, y2].\n","    \"\"\"\n","    xi1 = max(box1[0], box2[0])\n","    yi1 = max(box1[1], box2[1])\n","    xi2 = min(box1[2], box2[2])\n","    yi2 = min(box1[3], box2[3])\n","    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n","    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n","    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n","    union_area = box1_area + box2_area - inter_area\n","    return inter_area / union_area if union_area > 0 else 0\n","\n","def compute_ap_image(detections, gt_boxes, iou_threshold=0.5):\n","    \"\"\"\n","    Computes a simple per-image Average Precision (AP) for detections.\n","    1. Sort detections by confidence.\n","    2. Assign a true positive if IoU with an unassigned ground truth box >= iou_threshold.\n","    3. Compute precision and recall, then approximate AP using the trapezoidal rule.\n","    \"\"\"\n","    if len(gt_boxes) == 0 or len(detections[\"scores\"]) == 0:\n","        return 0.0\n","\n","    sorted_inds = np.argsort(-detections[\"scores\"])\n","    boxes = detections[\"boxes\"][sorted_inds]\n","    scores = detections[\"scores\"][sorted_inds]\n","\n","    tp = np.zeros(len(boxes))\n","    fp = np.zeros(len(boxes))\n","    matched = np.zeros(len(gt_boxes), dtype=bool)\n","\n","    for i, box in enumerate(boxes):\n","        best_iou = 0\n","        best_j = -1\n","        for j, gt in enumerate(gt_boxes):\n","            iou_val = iou(box, gt)\n","            if iou_val > best_iou:\n","                best_iou = iou_val\n","                best_j = j\n","        if best_iou >= iou_threshold and not matched[best_j]:\n","            tp[i] = 1\n","            matched[best_j] = True\n","        else:\n","            fp[i] = 1\n","\n","    cum_tp = np.cumsum(tp)\n","    cum_fp = np.cumsum(fp)\n","    precisions = cum_tp / (cum_tp + cum_fp + 1e-6)\n","    recalls = cum_tp / (len(gt_boxes) + 1e-6)\n","\n","    ap = np.trapz(precisions, recalls)\n","    return ap\n","\n","# -------------------------------\n","# 2. Visibility Classifier & Its Functions\n","# -------------------------------\n","\n","class VisibilityClassifier(nn.Module):\n","    def __init__(self, num_classes=5):  # classes: fog, rain, sand, snow, good_condition_best\n","        super(VisibilityClassifier, self).__init__()\n","        self.model = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(128*128*3, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, num_classes)\n","        )\n","    def forward(self, x):\n","        return self.model(x)\n","\n","def load_visibility_model(model_path):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model = VisibilityClassifier().to(device)\n","    state_dict = torch.load(model_path, map_location=device)\n","    new_state_dict = { k if k.startswith(\"model.\") else f\"model.{k}\": v for k, v in state_dict.items() }\n","    model.load_state_dict(new_state_dict)\n","    model.eval()\n","    return model\n","\n","def classify_visibility(image_path, model):\n","    transform = transforms.Compose([\n","        transforms.Resize((128, 128)),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485,0.456,0.406],\n","                             [0.229,0.224,0.225])\n","    ])\n","    image = Image.open(image_path).convert(\"RGB\")\n","    image = transform(image).unsqueeze(0)\n","    image = image.to(next(model.parameters()).device)\n","    with torch.no_grad():\n","        output = model(image)\n","        pred = torch.argmax(output, dim=1).item()\n","    condition_map = [\"fog\", \"rain\", \"sand\", \"snow\", \"good_condition_best\"]\n","    return condition_map[pred]\n","\n","# -------------------------------\n","# 3. YOLO Model Loading\n","# -------------------------------\n","\n","BASE_WEIGHTS_PATH = \"/content/drive/MyDrive/DEEP_LEARNING/FinalProject/datasets/DAWN/weights\"\n","FILE_MAPPING = {\n","    \"fog\": (\"yolov8_fog\", \"yolov8_fog_best.pt\"),\n","    \"rain\": (\"yolov8_rain\", \"yolov8_rain_best.pt\"),\n","    \"sand\": (\"yolov8_sand\", \"yolov8_sand_best.pt\"),\n","    \"snow\": (\"yolov8_snow\", \"yolov8_snow_best.pt\"),\n","    \"good_condition_best\": (\"yolov8_good_condition\", \"yolov8_good_condition_best.pt\")\n","}\n","\n","def load_submodel(condition):\n","    if condition not in FILE_MAPPING:\n","        raise ValueError(f\"No submodel mapping for condition={condition}\")\n","    folder, filename = FILE_MAPPING[condition]\n","    model_path = os.path.join(BASE_WEIGHTS_PATH, folder, filename)\n","    if not os.path.exists(model_path):\n","        raise FileNotFoundError(f\"Submodel not found at {model_path}\")\n","    return YOLO(model_path)\n","\n","# Load the general model (trained on all conditions)\n","GENERAL_MODEL_PATH = os.path.join(BASE_WEIGHTS_PATH, \"yolov8_good_condition\", \"yolov8_good_condition_best.pt\")\n","general_model = YOLO(GENERAL_MODEL_PATH)\n","\n","# -------------------------------\n","# 4. YOLO Inference Function\n","# -------------------------------\n","\n","def run_inference(image_path, model):\n","    results = model(image_path, verbose=False)\n","    if results[0].boxes is not None:\n","        boxes = results[0].boxes.xyxy.cpu().numpy()\n","        conf = results[0].boxes.conf.cpu().numpy()\n","        cls_ids = results[0].boxes.cls.cpu().numpy().astype(int)\n","    else:\n","        boxes, conf, cls_ids = [], [], []\n","    return {\"boxes\": boxes, \"scores\": conf, \"class_ids\": cls_ids}\n","\n","# -------------------------------\n","# 5. Visualization Functions\n","# -------------------------------\n","\n","def visualize_comparison_grid(image_paths, sub_detections, gen_detections, sub_model_names, gen_model_names, condition, title_suffix=\"\"):\n","    \"\"\"\n","    Creates a grid with 2 rows and N columns:\n","      - Top row: images with submodel detections.\n","      - Bottom row: same images with general model detections.\n","    \"\"\"\n","    N = len(image_paths)\n","    plt.figure(figsize=(4 * N, 8))\n","    for i, img_path in enumerate(image_paths):\n","        orig = cv2.imread(img_path)\n","        orig_rgb = cv2.cvtColor(orig, cv2.COLOR_BGR2RGB)\n","        img_sub = orig_rgb.copy()\n","        img_gen = orig_rgb.copy()\n","\n","        # Draw submodel detections (green)\n","        for bbox, score, cls in zip(sub_detections[i][\"boxes\"], sub_detections[i][\"scores\"], sub_detections[i][\"class_ids\"]):\n","            x1, y1, x2, y2 = map(int, bbox)\n","            label = f\"{sub_model_names[cls]}: {score:.2f}\"\n","            cv2.rectangle(img_sub, (x1, y1), (x2, y2), (0, 255, 0), 2)\n","            cv2.putText(img_sub, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n","\n","        # Draw general model detections (red)\n","        for bbox, score, cls in zip(gen_detections[i][\"boxes\"], gen_detections[i][\"scores\"], gen_detections[i][\"class_ids\"]):\n","            x1, y1, x2, y2 = map(int, bbox)\n","            label = f\"{gen_model_names[cls]}: {score:.2f}\"\n","            cv2.rectangle(img_gen, (x1, y1), (x2, y2), (255, 0, 0), 2)\n","            cv2.putText(img_gen, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n","\n","        plt.subplot(2, N, i+1)\n","        plt.imshow(img_sub)\n","        plt.title(f\"Submodel ({condition}) {title_suffix}\")\n","        plt.axis(\"off\")\n","\n","        plt.subplot(2, N, N + i+1)\n","        plt.imshow(img_gen)\n","        plt.title(f\"General Model {title_suffix}\")\n","        plt.axis(\"off\")\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","def select_and_visualize_top_difference(condition, images_folder, labels_folder, num_examples=2, iou_threshold=0.5):\n","    \"\"\"\n","    For a given condition, iterates over all images in the validation folder,\n","    computes the per-image AP for both the sub-model and the general model,\n","    calculates the difference (sub-model AP - general model AP),\n","    and selects the top 'num_examples' images with the largest positive differences.\n","    Then displays them in a side-by-side grid.\n","    \"\"\"\n","    sub_model = load_submodel(condition)\n","    sub_names = sub_model.names\n","    gen_names = general_model.names\n","\n","    image_list = [os.path.join(images_folder, f) for f in os.listdir(images_folder) if f.lower().endswith(\".jpg\")]\n","\n","    differences = []\n","    for img in image_list:\n","        det_sub = run_inference(img, sub_model)\n","        det_gen = run_inference(img, general_model)\n","\n","        # Load ground truth boxes from the corresponding label file\n","        img_cv = cv2.imread(img)\n","        if img_cv is None:\n","            continue\n","        h, w = img_cv.shape[:2]\n","        label_file = os.path.join(labels_folder, os.path.basename(img).replace(\".jpg\", \".txt\"))\n","        gt_boxes = load_gt_boxes(label_file, (w, h))\n","\n","        ap_sub = compute_ap_image(det_sub, gt_boxes, iou_threshold)\n","        ap_gen = compute_ap_image(det_gen, gt_boxes, iou_threshold)\n","        diff = ap_sub - ap_gen\n","        differences.append((img, diff, det_sub, det_gen))\n","\n","    # Filter for images where the sub-model has a higher AP\n","    differences = [d for d in differences if d[1] > 0]\n","    # Sort by difference (largest first)\n","    differences.sort(key=lambda x: x[1], reverse=True)\n","\n","    best = differences[:num_examples]\n","    if not best:\n","        print(f\"No images found where {condition} submodel outperforms the general model in AP.\")\n","        return\n","    best_imgs = [x[0] for x in best]\n","    sub_dets = [x[2] for x in best]\n","    gen_dets = [x[3] for x in best]\n","\n","    visualize_comparison_grid(best_imgs, sub_dets, gen_dets, sub_names, gen_names, condition, title_suffix=\"(Top AP Difference)\")\n","\n","# -------------------------------\n","# Main Pipeline: Process All Conditions\n","# -------------------------------\n","\n","# Define validation folders for each condition (images and labels)\n","CONDITIONS = {\n","    \"fog\": {\n","        \"images\": \"/content/drive/MyDrive/DEEP_LEARNING/FinalProject/datasets/DAWN/Fog/images/val\",\n","        \"labels\": \"/content/drive/MyDrive/DEEP_LEARNING/FinalProject/datasets/DAWN/Fog/labels/val\"\n","    },\n","    \"rain\": {\n","        \"images\": \"/content/drive/MyDrive/DEEP_LEARNING/FinalProject/datasets/DAWN/Rain/images/val\",\n","        \"labels\": \"/content/drive/MyDrive/DEEP_LEARNING/FinalProject/datasets/DAWN/Rain/labels/val\"\n","    },\n","    \"sand\": {\n","        \"images\": \"/content/drive/MyDrive/DEEP_LEARNING/FinalProject/datasets/DAWN/Sand/images/val\",\n","        \"labels\": \"/content/drive/MyDrive/DEEP_LEARNING/FinalProject/datasets/DAWN/Sand/labels/val\"\n","    },\n","    \"snow\": {\n","        \"images\": \"/content/drive/MyDrive/DEEP_LEARNING/FinalProject/datasets/DAWN/Snow/images/val\",\n","        \"labels\": \"/content/drive/MyDrive/DEEP_LEARNING/FinalProject/datasets/DAWN/Snow/labels/val\"\n","    },\n","    \"good_condition_best\": {\n","        \"images\": \"/content/drive/MyDrive/DEEP_LEARNING/FinalProject/datasets/DAWN/good_condition/images/val\",\n","        \"labels\": \"/content/drive/MyDrive/DEEP_LEARNING/FinalProject/datasets/DAWN/good_condition/labels/val\"\n","    }\n","}\n","\n","# For each condition, select and display the top 2 images\n","for cond, paths in CONDITIONS.items():\n","    print(f\"\\n--- Visualizing top 2 images for condition: {cond.upper()} (Highest AP Difference) ---\")\n","    select_and_visualize_top_difference(cond, paths[\"images\"], paths[\"labels\"], num_examples=10, iou_threshold=0.5)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":691,"output_embedded_package_id":"1cYhsZK7mmGuxuQhi9TPUzVNlvrKgzTEz"},"id":"4Jmce41gn8RN","executionInfo":{"status":"ok","timestamp":1741438839302,"user_tz":-120,"elapsed":124513,"user":{"displayName":"Yarin Edri","userId":"16106484749537069937"}},"outputId":"ae8f1d57-6517-4dd9-e8f5-17ace5ab26ec"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyPi9eudbPTyjnoJUjvlsg49"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}